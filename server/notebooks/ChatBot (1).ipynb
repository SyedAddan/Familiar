{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1HvyvUHCecLk_q_znkPlO8bVnBoPNjVKg","authorship_tag":"ABX9TyPx83Uwx1o7aSPAu6BCWl+e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BkoYts6D4gG6","executionInfo":{"status":"ok","timestamp":1694028037629,"user_tz":-300,"elapsed":18653,"user":{"displayName":"i202438 Khawaja Muhammad Hassam Nazir","userId":"07532651612098312979"}},"outputId":"7e3287a7-b8ef-423a-935e-c5968ecbb963"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.1\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n"]}],"source":["# !pip install transformers\n","# !pip install soundfile"]},{"cell_type":"code","source":["import soundfile as sf\n","from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n","import torch\n","audio_file = \"/content/Recording.wav\"\n","\n","# Load the audio signal\n","waveform, sample_rate = sf.read(audio_file)\n","\n","# Resample audio to match expected sampling rate of the model\n","if sample_rate != 16000:\n","    # Resample the waveform using scipy.signal.resample\n","    from scipy import signal\n","    waveform = signal.resample(waveform, int(waveform.shape[0] * 16000 / sample_rate))\n","    sample_rate = 16000\n","\n","# Check that the input audio signal has the expected shape\n","if waveform.ndim != 1:\n","    waveform = waveform[:, 0]\n","\n","# Check that the audio signal is preprocessed according to the model's requirements\n","if (waveform.max() > 1) or (waveform.min() < -1):\n","    waveform = waveform / max(abs(waveform))\n","#Initialize the model and tokenizer\n","model = Wav2Vec2ForCTC.from_pretrained(\"/content/drive/MyDrive/Pretrained_models/wav2\")\n","tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"/content/drive/MyDrive/Pretrained_models/wav2tok\")\n","\n","# Encode the input audio as a sequence of tokens\n","input_values = tokenizer(waveform, return_tensors=\"pt\").input_values.squeeze()\n","\n","# Perform transcription\n","with torch.no_grad():\n","    logits = model(input_values.unsqueeze(0)).logits\n","predicted_ids = torch.argmax(logits, dim=-1)\n","transcription = tokenizer.decode(predicted_ids[0])\n","print(transcription)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nsOw5AP85MMP","executionInfo":{"status":"ok","timestamp":1694029528207,"user_tz":-300,"elapsed":13858,"user":{"displayName":"i202438 Khawaja Muhammad Hassam Nazir","userId":"07532651612098312979"}},"outputId":"0e341b12-8084-4797-865c-f0f4e197d68b"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["HELLO HOW ARE YOU WHAT ARE YOU DOING\n"]}]},{"cell_type":"code","source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","import torch\n","tokenizer = GPT2Tokenizer.from_pretrained(\"af1tang/personaGPT\")\n","model = GPT2LMHeadModel.from_pretrained(\"af1tang/personaGPT\")\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","## utility functions ##\n","flatten = lambda l: [item for sublist in l for item in sublist]\n","\n","def to_data(x):\n","    if torch.cuda.is_available():\n","        x = x.cpu()\n","    return x.data.numpy()\n","\n","def to_var(x):\n","    if not torch.is_tensor(x):\n","        x = torch.Tensor(x)\n","    if torch.cuda.is_available():\n","        x = x.cuda()\n","    return x\n","\n","def display_dialog_history(dialog_hx):\n","    for j, line in enumerate(dialog_hx):\n","        msg = tokenizer.decode(line)\n","        if j %2 == 0:\n","            print(\">> User: \"+ msg)\n","        else:\n","            print(\"Bot: \"+msg)\n","            print()\n","\n","def generate_next(bot_input_ids, do_sample=True, top_k=10, top_p=.92,\n","                  max_length=1000, pad_token=tokenizer.eos_token_id):\n","    full_msg = model.generate(bot_input_ids, do_sample=True,\n","                                              top_k=top_k, top_p=top_p,\n","                                              max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n","    msg = to_data(full_msg.detach()[0])[bot_input_ids.shape[-1]:]\n","    return msg\n"],"metadata":{"id":"cDOjAnGY54SV","executionInfo":{"status":"ok","timestamp":1694029545956,"user_tz":-300,"elapsed":12167,"user":{"displayName":"i202438 Khawaja Muhammad Hassam Nazir","userId":"07532651612098312979"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# get personality facts for conversation\n","personas = []\n","for i in range(1):\n","    response = input(\">> Fact %d: \"%(i+1))+ tokenizer.eos_token\n","    personas.append(response)\n","personas = tokenizer.encode(''.join(['<|p2|>'] + personas + ['<|sep|>'] + ['<|start|>']))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7uIsU-RT8hVd","executionInfo":{"status":"ok","timestamp":1694029574068,"user_tz":-300,"elapsed":14094,"user":{"displayName":"i202438 Khawaja Muhammad Hassam Nazir","userId":"07532651612098312979"}},"outputId":"1e7d66ea-03fe-4578-9cd1-41748645d61b"},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":[">> Fact 1: general chatting between friends\n"]}]},{"cell_type":"code","source":["# converse for 8 turns\n","dialog_hx = []\n","for step in range(1):\n","    # encode the user input\n","    user_inp = tokenizer.encode(\">> User: \" + transcription + tokenizer.eos_token)\n","    # append to the chat history\n","    dialog_hx.append(user_inp)\n","\n","    # generated a response while limiting the total chat history to 1000 tokens,\n","    bot_input_ids = to_var([personas + flatten(dialog_hx)]).long()\n","    msg = generate_next(bot_input_ids)\n","    dialog_hx.append(msg)\n","    print(\"Bot: {}\".format(tokenizer.decode(msg, skip_special_tokens=True)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FROPc1bzAaMV","executionInfo":{"status":"ok","timestamp":1694029579092,"user_tz":-300,"elapsed":718,"user":{"displayName":"i202438 Khawaja Muhammad Hassam Nazir","userId":"07532651612098312979"}},"outputId":"e23ed85d-9c09-4e9b-8f14-57d285b36501"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["Bot: hello i'm doing great how are you?\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"3KjvYpE6AqNM"},"execution_count":null,"outputs":[]}]}